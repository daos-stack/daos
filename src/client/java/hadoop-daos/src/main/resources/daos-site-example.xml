<configuration>
  <property>
    <name>fs.defaultFS</name>
    <value>daos:///</value>
    <description>Unique DAOS server URI which follows standard URI format, schema://[authority]/. 'schema' is fixed
      'daos'. And '[authority]' is optional depending on your choices. Different from other file systems, pool UUID and
      container UUID are needed to connect to DAOS instead of IP/port. And long UUIDs are not good choice put as URI
      authority. Thus a mapping between authority and UUIDs is given here for simplicity if authority is necessary. Here
      is the convention.

      1, default URI without authority, 'daos:///'. It reads default values of "fs.daos.pool.uuid" and
      "fs.daos.container.uuid". And try to read other optional configs prefixed with "fs.daos." from this file. Be sure
      there is no DAOS URI with authority, like 'daos://fs/', configured to "fs.DefaultFS". Otherwise, you'll get
      filesystem instance of 'daos://fs/' instead of your desired one, 'daos:///', after you called the Hadoop
      FileSystem get methods, like 'get(uri, configuration)'.

      2, URI with authority, like 'daos://fs1/'. It reads values of "fs1.fs.daos.pool.uuid" and
      "fs1.fs.daos.container.uuid" first. And try to read other optional configs from "fs1.fs.daos.*" first and
      fall back to optional configs prefixed with "fs.daos." from this file.

      The two UUID configs must be provided whilst the others are all optional. If this file is not provided in
      classpath or the UUIDs are not configured, user should at least set "fs.daos.pool.uuid" and
      "fs.daos.container.uuid" in Hadoop Configuration instance passed for getting file system. Configs from Hadoop have
      higher priority than this file, including the UUIDs. And you should not prefix the configs with authority when set
      them in Hadoop Configuration even URI has authority.
    </description>
  </property>
  <property>
    <name>fs.daos.server.group</name>
    <value>daos_server</value>
    <description>daos server group</description>
  </property>
  <property>
    <name>fs.daos.pool.uuid</name>
    <value>uuid of pool</value>
    <description>UUID of DAOS pool</description>
  </property>
  <property>
    <name>fs.daos.pool.flags</name>
    <value>2</value>
    <description>daos pool access flags, 1 for readonly, 2 for read/write, 4 for execute</description>
  </property>
  <property>
    <name>fs.daos.container.uuid</name>
    <value>uuid of container</value>
    <description>UUID of DAOS container which created with "--type posix"</description>
  </property>
  <property>
    <name>fs.daos.pool.svc</name>
    <value>0</value>
    <description>service list separated by ":" if more than one service</description>
  </property>
  <property>
    <name>fs.daos.read.buffer.size</name>
    <value>8388608</value>
    <description>size of direct buffer for reading data from DAOS. Default is 8m.
      Value range is 64k - 2g.
    </description>
  </property>
  <property>
    <name>fs.daos.write.buffer.size</name>
    <value>8388608</value>
    <description>size of direct buffer for writing data to DAOS. Default is 8m.
      Value range is 64k - 2g.
    </description>
  </property>
  <property>
    <name>fs.daos.block.size</name>
    <value>134217728</value>
    <description>
      size for splitting large file into blocks when read by Hadoop. Default is 128m.
      Value range is 16m - 2m.
    </description>
  </property>
  <property>
    <name>fs.daos.chunk.size</name>
    <value>1048576</value>
    <description>
      size of DAOS file chunk. Default is 1m.
      Value range is 4k - 2g.
    </description>
  </property>
  <property>
    <name>fs.daos.preload.size</name>
    <value>4194304</value>
    <description>
      size for pre-loading more than requested data from DAOS into internal buffer when read.
      value should not be greater than "fs.daos.read.buffer.size".
      Maximum is 2g.
    </description>
  </property>
</configuration>